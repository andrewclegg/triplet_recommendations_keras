{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations in Keras using triplet loss\n",
    "Along the lines of BPR [1]. \n",
    "\n",
    "[1] Rendle, Steffen, et al. \"BPR: Bayesian personalized ranking from implicit feedback.\" Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009.\n",
    "\n",
    "This is implemented (more efficiently) in LightFM (https://github.com/lyst/lightfm). See the MovieLens example (https://github.com/lyst/lightfm/blob/master/examples/movielens/example.ipynb) for results comparable to this notebook.\n",
    "\n",
    "## Set up the architecture\n",
    "A simple dense layer for both users and items: this is exactly equivalent to latent factor matrix when multiplied by binary user and item indices. There are three inputs: users, positive items, and negative items. In the triplet objective we try to make the positive item rank higher than the negative item for that user.\n",
    "\n",
    "Because we want just one single embedding for the items, we use shared weights for the positive and negative item inputs (a siamese architecture).\n",
    "\n",
    "This is all very simple but could be made arbitrarily complex, with more layers, conv layers and so on. I expect we'll be seeing a lot of papers doing just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Triplet loss network example for recommenders\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, Lambda\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import data\n",
    "import metrics\n",
    "\n",
    "\n",
    "class BprLoss(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(BprLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(BprLoss, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        assert len(inputs) == 3\n",
    "        pos_item = inputs[0]\n",
    "        neg_item = inputs[1]\n",
    "        user = inputs[2]\n",
    "        \n",
    "        loss = 1.0 - K.sigmoid(\n",
    "            K.sum(user * pos_item, axis=-1, keepdims=True) -\n",
    "            K.sum(user * neg_item, axis=-1, keepdims=True))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n",
    "\n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    \n",
    "    # compute mask-aware mean of x, or all zeroes if no rows present\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False) / n\n",
    "    x_mean = tf.check_numerics(\n",
    "        x_mean,\n",
    "        'unexpected nans found in mean -- check at least one entry is present')\n",
    "\n",
    "    return x_mean\n",
    "\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "\n",
    "    return K.mean(y_pred * y_true)\n",
    "\n",
    "\n",
    "def build_model(num_users, num_items, num_tags, max_tags,\n",
    "                item_latent_dim, tag_latent_dim):\n",
    "\n",
    "    # ID vectors for the positive and negative items\n",
    "    positive_item_input = Input((1, ), name='positive_item_input')\n",
    "    negative_item_input = Input((1, ), name='negative_item_input')\n",
    "    \n",
    "    # Zero-padded tag ID vectors, for the positive and negative items\n",
    "    positive_tags = Input((max_tags, ), name='positive_tags')\n",
    "    negative_tags = Input((max_tags, ), name='negative_tags')\n",
    "\n",
    "    # Shared embedding layer for positive and negative items\n",
    "    item_embedding_layer = Embedding(\n",
    "        num_items, item_latent_dim, name='item_embedding', input_length=1)\n",
    "    \n",
    "    # Shared embedding layer for positive and negative items' tags\n",
    "    tag_embedding_layer = Embedding(\n",
    "        num_tags, tag_latent_dim, name='tag_embedding', input_length=max_tags)\n",
    "\n",
    "    user_input = Input((1, ), name='user_input')\n",
    "\n",
    "    positive_item_embedding = Flatten()(item_embedding_layer(\n",
    "        positive_item_input))\n",
    "    \n",
    "    negative_item_embedding = Flatten()(item_embedding_layer(\n",
    "        negative_item_input))\n",
    "    \n",
    "    positive_tags_embedding = Lambda(\n",
    "        mask_aware_mean, mask_aware_mean_output_shape, name='pos_mean')(tag_embedding_layer(positive_tags))\n",
    "    \n",
    "    negative_tags_embedding = Lambda(\n",
    "        mask_aware_mean, mask_aware_mean_output_shape, name='neg_mean')(tag_embedding_layer(negative_tags))\n",
    "    \n",
    "    positive_vec = concatenate([positive_item_embedding, positive_tags_embedding])\n",
    "    \n",
    "    negative_vec = concatenate([negative_item_embedding, negative_tags_embedding])\n",
    "    \n",
    "    # User embedding has to have dimensionality equal to item plus tag embeddings,\n",
    "    # as they need to align element-wise\n",
    "    user_latent_dim = item_latent_dim + tag_latent_dim\n",
    "    user_embedding = Flatten()(Embedding(\n",
    "        num_users, user_latent_dim, name='user_embedding', input_length=1)(\n",
    "            user_input))\n",
    "\n",
    "    loss = BprLoss(name='bpr_loss')([positive_vec, negative_vec, user_embedding])\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[positive_item_input, positive_tags, negative_item_input, negative_tags, user_input],\n",
    "        outputs=loss)\n",
    "    model.compile(loss=identity_loss, optimizer=Adam())\n",
    "    \n",
    "    # Now define a separate model for prediction, only using one half of the\n",
    "    # siamese network, plus the user\n",
    "    \n",
    "    user_dot_item = dot(\n",
    "        [positive_vec, user_embedding], axes=-1, name='user_dot_item')\n",
    "    \n",
    "    pred_model = Model(\n",
    "        inputs=[positive_item_input, positive_tags, user_input],\n",
    "        outputs=user_dot_item)\n",
    "\n",
    "    return model, pred_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform data\n",
    "We're going to load the Movielens 100k dataset and create triplets of (user, known positive item, randomly sampled negative item).\n",
    "\n",
    "The success metric is AUC: in this case, the probability that a randomly chosen known positive item from the test set is ranked higher for a given user than a ranomly chosen negative item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "item_latent_dim = 90\n",
    "tag_latent_dim = 10\n",
    "\n",
    "# Read data\n",
    "train, test = data.get_movielens_data()\n",
    "num_users, num_items = train.shape\n",
    "\n",
    "item_features = data.get_movielens_item_metadata(use_item_ids=False)\n",
    "\n",
    "max_tags = item_features.shape[1]\n",
    "num_tags = item_features.max() + 1\n",
    "\n",
    "# Prepare the test triplets\n",
    "test_uid, test_pid, test_nid = data.get_triplets(test)\n",
    "\n",
    "model, pred_model = build_model(\n",
    "    num_users, num_items, num_tags, max_tags,\n",
    "    item_latent_dim, tag_latent_dim)\n",
    "\n",
    "# Print the model structure\n",
    "print('Model for training:')\n",
    "print(model.summary())\n",
    "print()\n",
    "print('Model for inference:')\n",
    "print(pred_model.summary())\n",
    "print()\n",
    "\n",
    "# Sanity check, should be around 0.5\n",
    "print('AUC before training %s' % metrics.full_auc(pred_model, test, item_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write metadata for TensorBoard\n",
    "\n",
    "### TODO move this stuff to data.py (or maybe metadata.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "log_dir = '/tmp/tfboard/triplet_keras/'\n",
    "\n",
    "shutil.rmtree(log_dir, onerror=lambda f, p, e: print(e))\n",
    "\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except Exception, e:\n",
    "    print(str(e))\n",
    "\n",
    "items_metadata = os.path.join(log_dir, 'items.txt')\n",
    "with open(items_metadata, 'w') as f:\n",
    "    print('0 - None', file=f)\n",
    "    for line in data._get_movie_raw_metadata():\n",
    "        fields = line.split('|')\n",
    "        if len(fields) > 1:\n",
    "            print('%s - %s' % (fields[0], fields[1]), file=f)\n",
    "\n",
    "tags_metadata = os.path.join(log_dir, 'tags.txt')\n",
    "with open(tags_metadata, 'w') as f:\n",
    "    # No need for dummy '0' as above -- this is already provided\n",
    "    for line in data._get_genre_raw_metadata():\n",
    "        fields = line.split('|')\n",
    "        if len(fields) > 1:\n",
    "            # Note fields are opposite way round from movies\n",
    "            print('%s - %s' % (fields[1], fields[0]), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Run for a couple of epochs, checking the AUC after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "    log_dir='/tmp/tfboard/triplet_keras/',\n",
    "    embeddings_freq=1,\n",
    "    embeddings_layer_names=['item_embedding', 'tag_embedding', 'user_embedding'],\n",
    "    embeddings_metadata={'item_embedding': items_metadata, 'tag_embedding': tags_metadata})\n",
    "\n",
    "num_epochs = 30\n",
    "checkpoint_every = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print('Epoch %s' % epoch)\n",
    "\n",
    "    # Sample triplets from the training data\n",
    "    uid, pid, nid = data.get_triplets(train)\n",
    "    ptags = item_features[pid]\n",
    "    ntags = item_features[nid]\n",
    "\n",
    "    X = {\n",
    "        'user_input': uid,\n",
    "        'positive_item_input': pid,\n",
    "        'negative_item_input': nid,\n",
    "        'positive_tags': ptags,\n",
    "        'negative_tags': ntags\n",
    "    }\n",
    "    \n",
    "    checkpoint = ((epoch + 1) % checkpoint_every == 0)\n",
    "    \n",
    "    if checkpoint:\n",
    "        callbacks=[tensorboard]\n",
    "    else:\n",
    "        callbacks=[]\n",
    "\n",
    "    model.fit(X,\n",
    "              np.ones(len(uid)),\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    if checkpoint:\n",
    "        print('AUC %s' % metrics.full_auc(pred_model, test, item_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is in the low-90s. At some point we start overfitting, so it would be a good idea to stop early or add some regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
